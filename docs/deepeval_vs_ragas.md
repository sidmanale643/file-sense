# Dataset

### Total Generated Queries: 40

### LLM: “OpenAI gpt-4.1”

### Embedding Model: "BAAI/bge-base-en-v1.5”

# RAGAS

## Functionalities

1. Query Evolution
2. Query Synthesizers
    - Single Hop Abstract
    - Single Hop Specific
    - Multi Hop Abstract
    - Multi Hop Specific
3. Personas
4. Name Entity Extraction
5. Knowledge Graph

## Observations:

- Abstract queries feel realistic, unlike the somewhat lackluster ones generated by LLMs.
- Specific Queries feel realistic unlike LLM generated ones that are too too technical and feel extremely robotic
- Can create custom personas (Planner, Engineer and Citizen etc)
- Queries with typos included by default
- Specific queries slightly worse than Deep-Eval

---

# Deep-Eval

## Functionalities

## 1) Evolution:

Can select the weightage / probability for each (for eg: 1/7)

- REASONING
- MULTICONTEXT
- CONCRETIZING
- CONSTRAINED
- COMPARATIVE
- HYPOTHETICAL
- IN_BREADTH

## 2) Filtration Quality:

Provide a filtration threshold (for eg: 0.7), a critique model will  determine context quality score and all score below the threshold will be modified or discarded based on number of retries provided

## 3) Style Config:

The styles for:

- Queries
- Answer
- Task
- Scenario

can be modified by prompts

1. **Input Generation**: Generate synthetic queries `input`s with provided contexts.
2. **Filtration**: Filter away any initial synthetic queries that don't meet the specified generation standards.
3. **Evolution**: Evolve the filtered synthetic queries to increase complexity and make them more realistic.
4. **Styling**: Style the output formats of the `input`s and `expected_output`s of the evolved synthetic queries.

## Observations:

- Query Evolution is better than RAGAS
- Queries with typos missing
- General queries are sub-par
- Queries involving reasoning and multiple contexts better than RAGAS